{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "2316d3fe",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "# Julia for Economists\n",
    "## Parallelization for Fun and Profit\n",
    "### Cameron Pfiffer (cpfiffer@stanford.edu)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "fca3a85e",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "## Brief introduction\n",
    "\n",
    "- I am Cameron Pfiffer, howdy\n",
    "- Finance PhD student at University of Oregon\n",
    "- Visiting at the Stanford GSB\n",
    "- Work mostly on asset pricing, Bayesian econometrics, and computer go-fast stuff\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3011c71f",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "## What is this?\n",
    "\n",
    "- Second in a five-part series on Julia (don't worry if you missed the first)\n",
    "    3. Optimization and Automatic Differentiation (March)\n",
    "    4. Performant Programming and Best Practices (April)\n",
    "    5. Bayesian Inference and Probabilistic Programming (May)\n",
    "- The focus is programming better for research computing\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e543e2e5",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "## What are we going to do today?\n",
    "\n",
    "- Learn all about the CPU!\n",
    "- Multithreading\n",
    "- Multiprocessing\n",
    "- GPU parallelism\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7848f930",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "## How are we going to things do today?\n",
    "\n",
    "- 50 minute lectures.\n",
    "- 10 minute breaks at the top of the hour, 15 minutes at 11am.\n",
    "- Interrupt me to ask questions, or type them in the chat\n",
    "- Intermittent breaks for you to practice some code"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "39c23a48",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "## Preliminaries\n",
    "\n",
    "You will need\n",
    "\n",
    "1. A Julia installation\n",
    "2. A text editor (VS Code recommended)\n",
    "3. Your big beautiful brain\n",
    "\n",
    "If you do not have one of these **please take a moment to go get them**. \n",
    "\n",
    "You have a big beautiful brain, don't lie to me about that.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e4f195cc",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "## Warning!\n",
    "\n",
    "I learned a lot of this informally -- parallelization is typically a very dense and technical concept.\n",
    "\n",
    "I'm going to try my best to give you rigor when I can, but much of this will be a folksy introduction to parallel computing!\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a4437f3d",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "# Questions, comments, concerns?\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f43e611d",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "# The modern central processing unit and how amazing it is\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4b982785",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "## CPUs\n",
    "\n",
    "You might be aware that there's lots of little bits in your computer that do various kinds of magic.\n",
    "\n",
    "The CPU is easily one of the most important ones for research computing! And it is full of magic!\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "85b01e20",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "## CPUs\n",
    "\n",
    "CPUs are fancy boxes that accept\n",
    "\n",
    "1. _Instructions_ on what to do (machine code)\n",
    "2. _Data_ to use for the instructions (numbers, pixels, etc.)\n",
    "\n",
    "and perform the many requested tasks we ask them to do.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "dceb2612",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "## What's inside your CPU (the important conceptual ones)?\n",
    "\n",
    "- The __control unit (CU)__, which turns binary into timing and control signals.\n",
    "- The __arithmatic logic unit (ALU)__, which does \"the math\".\n",
    "- The __cache__, which is a place to store data and instructions for quick access. The cache stores quick-to-access versions of what is in main memory (RAM).\n",
    "- The __registers__, tiny little caches that the ALU can get to directly.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "cac12145",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "## What is inside your CPU?\n",
    "\n",
    "![](https://www.redhat.com/sysadmin/sites/default/files/styles/embed_large/public/2020-07/Figure2.png?itok=mQGilPbJ)\n",
    "\n",
    "[Image source](https://www.redhat.com/sysadmin/cpu-components-functionality)."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6aebb806",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "## What is a \"core\"\n",
    "\n",
    "CPUs are composed of one or more smaller **cores**. For example, a dual-core processor has two ALUs, one or more CUs, and may have caches unique to them (though commonly L2 or L3 are shared).\n",
    "\n",
    "Multiple cores can do totally differently things without impacting each other too much!"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "855964dc",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "## The cache\n",
    "\n",
    "The cache is short-term storage for instructions and data. It follows an organizational hierarchy:\n",
    "\n",
    "- L1 is a small cache that is very close to the processor. It is _really_ fast.\n",
    "- L2 is a larger cache that's a bit slower.\n",
    "- L3 is a specialzed cache designed to make L1 and L2 work better.\n",
    "\n",
    "When the processor needs data, it starts with L1, then L2, then L3, and if it is not in the CPU cache, it will check main memory (RAM)."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6be77d62",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "## Checking main memory is slow\n",
    "\n",
    "Here's [a typical latency](https://www.technipages.com/what-is-the-cpu-cache) for L1, L2, L3, and main memory:\n",
    "\n",
    "- L1: 0.8 nanoseconds\n",
    "- L2: 2.4 nanoseconds\n",
    "- L3: 11.1 nanoseconds\n",
    "- Main memory: 14 nanoseconds\n",
    "\n",
    "This is a long time! If stuff is already in your processor, it will be fast. If it has to go outside the cache, you may have to wait."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2b0e068b",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "This is why you may have heard of _vectorization_ being fast, especially if you come from an R world where vectoriazation is a good way to get free speed.\n",
    "\n",
    "Vectorization means you can keep the instruction (`sin`, for example) in L1 (or the register) and pump data through, meaning you don't have to do cache checks for new instructions."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7eef6411",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "## CPU internals (cache misses)\n",
    "\n",
    "For our purposes, the important parts are the three cache levels (L1, L2, L3). \n",
    "\n",
    "These store instructions and data. When the ALU needs data to do some math (`x + y`), it has to ask the cache in order if it has the data available -- if not, you may need to go get it from RAM, which is very costly!\n",
    "\n",
    "Not having the data in a cache and having to retrieve it is called a _cache miss_, and can be an important computational cost. For our purposes, these costs are quite small and likely to be difficult to track down. \n",
    "\n",
    "Don't worry about them too much."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2fca749c",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "## Serial processing\n",
    "\n",
    "CPUs at their most basic level do these tasks in *serial*, meaning they do them one at a time.\n",
    "\n",
    "If we have a long list of things we want done, it will do them _in order_ -- the sixth task is only started after the fifth is done."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "fab188e9",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "## Serial processing\n",
    "\n",
    "If, for example, I ran the following\n",
    "\n",
    "```julia\n",
    "x = 0\n",
    "for i in 1:2\n",
    "    x = x + i\n",
    "end\n",
    "```\n",
    "\n",
    "we know we'll have `x=3` at the end, but we executed the pseudo-machine code\n",
    "\n",
    "```julia\n",
    "x = 0\n",
    "x = x + 1\n",
    "x = x + 2\n",
    "```\n",
    "\n",
    "This is the order of the code, and it will _always_ be the order of the code.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "788ec690",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "## Serial processing\n",
    "\n",
    "There's a couple wonderful things about serial processing:\n",
    "\n",
    "1. It is deterministic (always know what you're going to get)\n",
    "2. CPUs are super fast at serial processing\n",
    "3. Lots of complex hidden benefits (easier to cache instructions/data, etc.)\n",
    "\n",
    "Most things are serial by default. You have to do a bit of work to make things parallel/concurrent!"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6ab67935",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "## Serial processing\n",
    "\n",
    "The downside is that serial processing requires __unrelated tasks__ to be completed _in order_!\n",
    "\n",
    "This is fine if I'm doing something like\n",
    "\n",
    "```julia\n",
    "function f(a, b)\n",
    "    x = 0\n",
    "    y = 0\n",
    "    for i in 1:10\n",
    "        x += a * i\n",
    "        y += b * i\n",
    "    end\n",
    "    \n",
    "    return x, y\n",
    "end\n",
    "```\n",
    "\n",
    "since calculating `x = x + a*i` is hilariously cheap. \n",
    "\n",
    "Note that we do not update `y` without first updating `x`, even though the _values_ of `x` and `y` do not depend on each other (only `i`)."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "48d6a94a",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "## Serial processing\n",
    "\n",
    "Serial computation can be extremely costly if you are doing lots of costly things in order. Take the following:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "id": "0d9496e5",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "BenchmarkTools.Trial: 330 samples with 1 evaluation.\n",
       " Range \u001b[90m(\u001b[39m\u001b[36m\u001b[1mmin\u001b[22m\u001b[39m … \u001b[35mmax\u001b[39m\u001b[90m):  \u001b[39m\u001b[36m\u001b[1m13.140 ms\u001b[22m\u001b[39m … \u001b[35m28.532 ms\u001b[39m  \u001b[90m┊\u001b[39m GC \u001b[90m(\u001b[39mmin … max\u001b[90m): \u001b[39m0.00% … 3.31%\n",
       " Time  \u001b[90m(\u001b[39m\u001b[34m\u001b[1mmedian\u001b[22m\u001b[39m\u001b[90m):     \u001b[39m\u001b[34m\u001b[1m13.844 ms              \u001b[22m\u001b[39m\u001b[90m┊\u001b[39m GC \u001b[90m(\u001b[39mmedian\u001b[90m):    \u001b[39m0.00%\n",
       " Time  \u001b[90m(\u001b[39m\u001b[32m\u001b[1mmean\u001b[22m\u001b[39m ± \u001b[32mσ\u001b[39m\u001b[90m):   \u001b[39m\u001b[32m\u001b[1m15.153 ms\u001b[22m\u001b[39m ± \u001b[32m 2.780 ms\u001b[39m  \u001b[90m┊\u001b[39m GC \u001b[90m(\u001b[39mmean ± σ\u001b[90m):  \u001b[39m2.44% ± 3.72%\n",
       "\n",
       "  \u001b[39m█\u001b[39m▃\u001b[39m▁\u001b[39m▁\u001b[34m▂\u001b[39m\u001b[39m \u001b[39m \u001b[39m▁\u001b[39m▄\u001b[39m▁\u001b[39m \u001b[32m \u001b[39m\u001b[39m \u001b[39m \u001b[39m \u001b[39m \u001b[39m \u001b[39m \u001b[39m \u001b[39m \u001b[39m \u001b[39m \u001b[39m \u001b[39m \u001b[39m \u001b[39m \u001b[39m \u001b[39m \u001b[39m \u001b[39m \u001b[39m \u001b[39m \u001b[39m \u001b[39m▄\u001b[39m \u001b[39m \u001b[39m \u001b[39m \u001b[39m \u001b[39m \u001b[39m \u001b[39m \u001b[39m \u001b[39m \u001b[39m \u001b[39m \u001b[39m \u001b[39m \u001b[39m \u001b[39m \u001b[39m \u001b[39m \u001b[39m \u001b[39m \u001b[39m \u001b[39m \u001b[39m \u001b[39m \u001b[39m \u001b[39m \u001b[39m \n",
       "  \u001b[39m█\u001b[39m█\u001b[39m█\u001b[39m█\u001b[34m█\u001b[39m\u001b[39m▇\u001b[39m▆\u001b[39m█\u001b[39m█\u001b[39m█\u001b[39m▇\u001b[32m▆\u001b[39m\u001b[39m▇\u001b[39m▅\u001b[39m▅\u001b[39m▆\u001b[39m▄\u001b[39m▁\u001b[39m▁\u001b[39m▅\u001b[39m▄\u001b[39m▄\u001b[39m▄\u001b[39m▅\u001b[39m▅\u001b[39m▁\u001b[39m▁\u001b[39m▁\u001b[39m▁\u001b[39m▄\u001b[39m▁\u001b[39m▁\u001b[39m▁\u001b[39m█\u001b[39m▇\u001b[39m▆\u001b[39m▇\u001b[39m▅\u001b[39m▄\u001b[39m▅\u001b[39m▁\u001b[39m▄\u001b[39m▁\u001b[39m▄\u001b[39m▁\u001b[39m▅\u001b[39m▄\u001b[39m▄\u001b[39m▁\u001b[39m▅\u001b[39m▄\u001b[39m▁\u001b[39m▁\u001b[39m▁\u001b[39m▄\u001b[39m▁\u001b[39m▁\u001b[39m▁\u001b[39m▄\u001b[39m \u001b[39m▆\n",
       "  13.1 ms\u001b[90m      \u001b[39m\u001b[90mHistogram: \u001b[39m\u001b[90m\u001b[1mlog(\u001b[22m\u001b[39m\u001b[90mfrequency\u001b[39m\u001b[90m\u001b[1m)\u001b[22m\u001b[39m\u001b[90m by time\u001b[39m        24 ms \u001b[0m\u001b[1m<\u001b[22m\n",
       "\n",
       " Memory estimate\u001b[90m: \u001b[39m\u001b[33m15.49 MiB\u001b[39m, allocs estimate\u001b[90m: \u001b[39m\u001b[33m10\u001b[39m."
      ]
     },
     "execution_count": 35,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "using LinearAlgebra, BenchmarkTools\n",
    "\n",
    "function expensive()\n",
    "    X = randn(10_000, 100)\n",
    "    return X'X\n",
    "end\n",
    "\n",
    "function go()\n",
    "    A = expensive()\n",
    "    B = expensive() # Have to wait for the first call to finish!\n",
    "    return A + B\n",
    "end\n",
    "\n",
    "# @benchmark runs expensive a bunch of times to estimate\n",
    "# the computational run time.\n",
    "@benchmark go()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "55880f55",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "In the above function, we might want to compute `A` and `B` _separately_ at the same time (since they do not depend on one another) and then handle the sum component at the end.\n",
    "\n",
    "In other words, how to we get away from this serial computation paradigm? Is there a way so that I don't have to wait for an expensive function to complete before I move on to the next?"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9b7322b6",
   "metadata": {},
   "source": [
    "## Multiprocess & multithread parallelism\n",
    "\n",
    "There are two common ways of achieving computational parallelism. \n",
    "\n",
    "- Multithreading, where you use one or more threads as part of a _process_. All threads share memory with one another.\n",
    "- Multiprocess, where you create an entire new process that has its own memory footprint. The new process(es) may have multiple threads.\n",
    "\n",
    "Multithreading is cheaper and lighter, but multiprocess parallelism is more general and extensible.\n",
    "\n",
    "__I generally prefer to multithread where possible__ -- multiprocess programming is often a lot harder, though it can be a spectacular fit for a class of problems."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4f5fa05d",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "## Multithreading\n",
    "\n",
    "Multithreading is (one) way to alleviate the friction of having to wait for sequential expressions to complete.\n",
    "\n",
    "You can think of a thread as a friend you've asked to help you move -- you can give them top-level instructions such as \"move all the boxes from the living room into the truck\", which frees you up to go pack boxes in the bedroom."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ce5aacb2",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "## Multiprocessing\n",
    "\n",
    "To stick with the moving example, I might think of multiprocessing as hiring several moving companies! Each company has their own workers and goals, all (hypothetically) working independently from one another.\n",
    "\n",
    "I will get to this later -- but for now, let's focus on multithreading."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7659b1ca",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "\n",
    "\n",
    "## Starting Julia with threads\n",
    "\n",
    "In order to use any threading capabilities in Julia, you have to create a Julia process with threads at the start. How to set this depends on how you are using Julia:\n",
    "\n",
    "- **Command line**. Set the threads with `julia --threads=n_threads`, or default to the number of cores you have with `julia --threads=auto`.\n",
    "- **VS Code REPL**. Go to the Julia extension settings, search for \"threads\" and enter a number -- 4 is probably safe, or use the number of cores in your machine if you know how many you have.\n",
    "\n",
    "Don't launch a ton of threads! You'll start getting real slow if you generate too many, and you may even lock your machine up. `--threads=auto` is safest for most people.\n",
    "\n",
    "Once inside Julia, run `Threads.nthreads()` and check to see that the result is greater than one."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3a06799e",
   "metadata": {
    "slideshow": {
     "slide_type": "skip"
    }
   },
   "source": [
    "If in Jupyter, you will need to make a new kernel:\n",
    "\n",
    "```julia\n",
    "using IJulia\n",
    "IJulia.installkernel(\n",
    "    \"Julia multithreading\", \n",
    "    env=Dict(\"JULIA_NUM_THREADS\" => 4)\n",
    ")\n",
    "\n",
    "```"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "992568dc",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "Once we have multiple threads, we can do all kinds of cool stuff! \n",
    "\n",
    "Let's start with just seeing how to use the more common threading construct, a multithreaded `for` loop.\n",
    "\n",
    "To do this, you can simply put `Threads.@threads` in front of `for`, and Julia will dispatch threads to handle different components of the loop."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "id": "d4a462f1",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Thread ID: 3 | i: 5\n",
      "Thread ID: 1 | i: 1\n",
      "Thread ID: 2 | i: 3\n",
      "Thread ID: 1 | i: 2\n",
      "Thread ID: 2 | i: 4\n",
      "Thread ID: 4 | i: 6\n"
     ]
    }
   ],
   "source": [
    "# Threads.threadid() tells us the ID number of the thread currently doing a task.\n",
    "function thread_test()\n",
    "    Threads.@threads for i in 1:6\n",
    "        id = Threads.threadid()\n",
    "        println(\"Thread ID: \", id, \" | i: \", i)\n",
    "    end\n",
    "end\n",
    "\n",
    "thread_test()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a59ee3a8",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "## Extremely important thing to see!\n",
    "\n",
    "The order is not deterministic! \n",
    "\n",
    "Threads show up differently when you re-run the code, and they are assigned different parts of the looping variable.\n",
    "\n",
    "If you start working with threads, you need to be aware that you are assigning a large _bundle_ of tasks, and that you cannot guarantee the order in which the tasks are completed."
   ]
  }
 ],
 "metadata": {
  "celltoolbar": "Slideshow",
  "kernelspec": {
   "display_name": "Julia 1.6.5",
   "language": "julia",
   "name": "julia-1.6"
  },
  "language_info": {
   "file_extension": ".jl",
   "mimetype": "application/julia",
   "name": "julia",
   "version": "1.6.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
